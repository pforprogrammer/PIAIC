{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSdmzLoG_KaO",
        "outputId": "2b6135c3-0b90-4d52-ce76-d1d914d9d7cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-google-genai langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "try:\n",
        "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "    from langchain.chains import LLMChain\n",
        "    from langchain.prompts import PromptTemplate\n",
        "except ImportError as e:\n",
        "    print(\"Error: Required libraries are not installed.\")\n",
        "    print(\"Please ensure 'google-genai' and 'langchain' are installed in your environment.\")\n",
        "    raise e"
      ],
      "metadata": {
        "id": "KY0m8GzfIZ3o"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Gemini API client\n",
        "# Replace 'YOUR_GEMINI_API_KEY' with your actual Gemini API key from Google Cloud.\n",
        "\n",
        "from google.colab import userdata\n",
        "try:\n",
        "  GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "except Exception as e:\n",
        "    print(\"Error: Could not initialize GeminiClient.\")\n",
        "    raise e\n"
      ],
      "metadata": {
        "id": "oQwg8cKmJVgh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a LangChain prompt template\n",
        "TEMPLATE = \"\"\"\n",
        "You are an advanced AI assistant using Google Gemini AI.\n",
        "Answer user queries concisely, clearly, and effectively.\n",
        "\n",
        "User Query: {query}\n",
        "\n",
        "Your Response:\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    prompt_template = PromptTemplate(input_variables=[\"query\"], template=TEMPLATE)\n",
        "except Exception as e:\n",
        "    print(\"Error: Could not create LangChain prompt template.\")\n",
        "    raise e"
      ],
      "metadata": {
        "id": "H2JX3l_DJchJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
        "    model = \"gemini-2.0-flash-exp\",\n",
        "    temperature=0.2,\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "Uc1VI4SOKKyW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LangChain LLMChain\n",
        "try:\n",
        "    chain = LLMChain(llm=client, prompt=prompt_template)\n",
        "except Exception as e:\n",
        "    print(\"Error: Could not initialize LangChain LLMChain.\")\n",
        "    raise e"
      ],
      "metadata": {
        "id": "PRb4xf1HJ4dI"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to generate a response using LangChain\n",
        "def generate_response_with_chain(user_query):\n",
        "    \"\"\"\n",
        "    Generates a response using LangChain.\n",
        "    Args:\n",
        "        user_query (str): The input query from the user.\n",
        "    Returns:\n",
        "        str: The AI's response or an error message if the request fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = chain.invoke({\"query\": user_query})\n",
        "        return response['text']\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "Y74aKZANKyli"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to display the AI-generated response\n",
        "def display_response(response):\n",
        "    \"\"\"\n",
        "    Displays the AI-generated response.\n",
        "    Args:\n",
        "        response (str): The AI's response content.\n",
        "    \"\"\"\n",
        "    print(\"\\n===== AI Response =====\")\n",
        "    print(response)"
      ],
      "metadata": {
        "id": "vyKehkOeK33I"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Main function to execute the program\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Runs the Hello Gemini Project application.\n",
        "    - Prompts the user for input.\n",
        "    - Generates and displays an AI-powered response using LangChain or Gemini directly.\n",
        "    \"\"\"\n",
        "    print(\"Welcome to the Hello Gemini Project!\")\n",
        "    print(\"This application uses Google Gemini AI with LangChain for advanced capabilities.\")\n",
        "    print(\"Type 'exit' to quit the program.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"Enter your query: \").strip()\n",
        "        if user_query.lower() == \"exit\":\n",
        "            print(\"Thank you for using the Hello Gemini Project. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # print(user_query);\n",
        "        ai_response = generate_response_with_chain(user_query)\n",
        "\n",
        "        # Display the AI response\n",
        "        display_response(ai_response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YG4f7wwH_Qka",
        "outputId": "0d22d6db-7bef-4750-d397-e511608983fe"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Hello Gemini Project!\n",
            "This application uses Google Gemini AI with LangChain for advanced capabilities.\n",
            "Type 'exit' to quit the program.\n",
            "\n",
            "Enter your query: who is imran khan\n",
            "who is imran khan\n",
            "\n",
            "===== AI Response =====\n",
            "Imran Khan is a Pakistani politician and former cricketer who served as the 22nd Prime Minister of Pakistan from 2018 to 2022. He founded the political party Pakistan Tehreek-e-Insaf (PTI).\n",
            "\n",
            "Enter your query: exit\n",
            "Thank you for using the Hello Gemini Project. Goodbye!\n"
          ]
        }
      ]
    }
  ]
}